{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxcayumi/17GP/blob/master/Remote_Controlling_LMs_without_prompting_or_finetuning_(contains_old_WPS).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31sY4DvEyek"
      },
      "source": [
        "# Controlling LMs without prompting or finetuning\n",
        "\n",
        "This notebook contains initial exploration with using `GPT2-XL` with online value-modification via natural-language modification of its activations.\n",
        "\n",
        "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator. For `GPT-2-XL`, you need to select \"high RAM.\"</b>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "commit = \"08efeb9\" # Stable commit\n",
        "get_ipython().run_line_magic(magic_name='pip', line=f'install -U git+https://github.com/montemac/algebraic_value_editing.git@{commit}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj1GS_4E8VyR",
        "outputId": "e2d6a9af-5d01-475f-be4d-0b35d32dc3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/montemac/algebraic_value_editing.git@08efeb9\n",
            "  Cloning https://github.com/montemac/algebraic_value_editing.git (to revision 08efeb9) to /tmp/pip-req-build-q64ipitw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/montemac/algebraic_value_editing.git /tmp/pip-req-build-q64ipitw\n",
            "\u001b[33m  WARNING: Did not find branch or tag '08efeb9', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q 08efeb9\n",
            "  Resolved https://github.com/montemac/algebraic_value_editing.git to commit 08efeb9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283\n",
            "  Cloning https://github.com/montemac/TransformerLens.git (to revision 74575aeeb8cc0ac0c98a2a24014166bcde5df283) to /tmp/pip-install-pu0tyg2t/transformer-lens_65a56e14f3c0423da4b0c48a6f4b9e89\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/montemac/TransformerLens.git /tmp/pip-install-pu0tyg2t/transformer-lens_65a56e14f3c0423da4b0c48a6f4b9e89\n",
            "  Running command git rev-parse -q --verify 'sha^74575aeeb8cc0ac0c98a2a24014166bcde5df283'\n",
            "  Running command git fetch -q https://github.com/montemac/TransformerLens.git 74575aeeb8cc0ac0c98a2a24014166bcde5df283\n",
            "  Resolved https://github.com/montemac/TransformerLens.git to commit 74575aeeb8cc0ac0c98a2a24014166bcde5df283\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.1 in /usr/local/lib/python3.9/dist-packages (from algebraic-value-editing==0.2.0) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from algebraic-value-editing==0.2.0) (1.5.3)\n",
            "Collecting jaxtyping>=0.2.14\n",
            "  Downloading jaxtyping-0.2.15-py3-none-any.whl (20 kB)\n",
            "Collecting prettytable>=3.6.0\n",
            "  Downloading prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting funcy>=2.0\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting wandb==0.13.5\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=0.27.2\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->algebraic-value-editing==0.2.0) (4.5.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (8.1.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (67.6.1)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (2.27.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (5.9.4)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.13.5->algebraic-value-editing==0.2.0) (1.16.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->algebraic-value-editing==0.2.0) (0.40.0)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.27.2->algebraic-value-editing==0.2.0) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.4->algebraic-value-editing==0.2.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.4->algebraic-value-editing==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable>=3.6.0->algebraic-value-editing==0.2.0) (0.2.6)\n",
            "Collecting fancy-einsum<0.0.4,>=0.0.3\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting datasets<3.0.0,>=2.7.1\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich<13.0.0,>=12.6.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops<0.7.0,>=0.6.0\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.25.1\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (2023.4.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (9.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb==0.13.5->algebraic-value-editing==0.2.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb==0.13.5->algebraic-value-editing==0.2.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb==0.13.5->algebraic-value-editing==0.2.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb==0.13.5->algebraic-value-editing==0.2.0) (1.26.15)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=12.6.0->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (2.14.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.25.1->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.25.1->transformer-lens@ git+https://github.com/montemac/TransformerLens.git@74575aeeb8cc0ac0c98a2a24014166bcde5df283->algebraic-value-editing==0.2.0) (3.11.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.14->algebraic-value-editing==0.2.0) (6.3.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai>=0.27.2->algebraic-value-editing==0.2.0) (22.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.14->algebraic-value-editing==0.2.0) (3.15.0)\n",
            "Building wheels for collected packages: algebraic-value-editing, transformer-lens, pathtools\n",
            "  Building wheel for algebraic-value-editing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for algebraic-value-editing: filename=algebraic_value_editing-0.2.0-py3-none-any.whl size=16710 sha256=463cd9ec4f4c436ec6ecf7310d4ab601ce9e8f772d12d4d7d71cf9f314eef4da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zzb3hkaw/wheels/21/1d/22/3666a0a44867d8c5cdc863b26161ac6428a311460d1ad39248\n",
            "  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=78364 sha256=3b8204acdaa51e0bd7a7c6c695ffcf46d3198349f349034f0319c3775ad1c2b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/fa/82/0bacca13e56b83588fc9cfdecaa0279f44facd490c149f492c\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=0a3190c8671cd6f31fce96a8cd95dceaae80581c37fedf974b21bed25c5eb02d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built algebraic-value-editing transformer-lens pathtools\n",
            "Installing collected packages: tokenizers, pathtools, funcy, commonmark, xxhash, smmap, shortuuid, setproctitle, sentry-sdk, rich, prettytable, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, frozenlist, fancy-einsum, einops, docker-pycreds, dill, async-timeout, yarl, typeguard, responses, nvidia-cudnn-cu11, multiprocess, huggingface-hub, gitdb, aiosignal, transformers, torch, jaxtyping, GitPython, aiohttp, wandb, openai, datasets, transformer-lens, algebraic-value-editing\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.3\n",
            "    Uninstalling rich-13.3.3:\n",
            "      Successfully uninstalled rich-13.3.3\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 0.7.2\n",
            "    Uninstalling prettytable-0.7.2:\n",
            "      Successfully uninstalled prettytable-0.7.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "ipython-sql 0.4.1 requires prettytable<1, but you have prettytable 3.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 algebraic-value-editing-0.2.0 async-timeout-4.0.2 commonmark-0.9.1 datasets-2.11.0 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.0 fancy-einsum-0.0.3 frozenlist-1.3.3 funcy-2.0 gitdb-4.0.10 huggingface-hub-0.13.4 jaxtyping-0.2.15 multidict-6.0.4 multiprocess-0.70.14 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-0.27.4 pathtools-0.1.2 prettytable-3.7.0 responses-0.18.0 rich-12.6.0 sentry-sdk-1.19.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 tokenizers-0.13.3 torch-1.13.1 transformer-lens-0.0.0 transformers-4.28.1 typeguard-3.0.2 wandb-0.13.5 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import List, Union, Tuple\n",
        "from functools import partial\n",
        "from transformer_lens.HookedTransformer import HookedTransformer\n",
        "\n",
        "from algebraic_value_editing.completion_utils import print_n_comparisons\n",
        "from algebraic_value_editing.prompt_utils import RichPrompt, get_x_vector"
      ],
      "metadata": {
        "id": "dNI_6C5I8dxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D4v3NMFEye1"
      },
      "source": [
        "## Loading the `HookedTransformer`\n",
        "\n",
        "In order to modify forward passes, we need `transformer_lens`'s activation cache functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8ddfbd8484984367a07f5cf5aaf4ed73",
            "226fed7521874289b839266af0cc4689",
            "38c890a3c7a74cb8872108f33968e31f",
            "93ffbfd51e5c48d09085154a9c163999",
            "d25f18fccf89404b9bef7b90cf7c1350",
            "ba70af8238294804b582e0048af76a1f",
            "05c15679a1164a9891e15f10f3790046",
            "fb66e1556cdd464484700dc758d7ddb4",
            "011133c729f34a029920122fb64d9c73",
            "1e47bb82aeae4f2b93fcbc6e05a39083",
            "1aca22e51d2343a797075ca277933d7c",
            "e3c1a61015b04acdbc1c7109aaec597a",
            "0e59eff156364ddeafc0b75241923110",
            "964f4bfb34474322be2bccf771645217",
            "72aa2c654da0480cb0bed1924d4f47de",
            "324e7032d2f44dc8b220a0fa0ca59837",
            "27ad3cad0ee34a73992434de7a3e4d01",
            "786c17d67aad433c908d05e05be296d4",
            "10fb005877cd44989895bcac99c0ce65",
            "d9c7eb943fc84395b5c9d18ba607661b",
            "152ad77c570c4f3d9ff48a9ce5fcedc4",
            "1dd36b399038413cb88d5e750eaa0c9c"
          ]
        },
        "id": "RTyNdTt3Eye3",
        "outputId": "c5652f54-a9da-430b-c164-a008632da741"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ddfbd8484984367a07f5cf5aaf4ed73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3c1a61015b04acdbc1c7109aaec597a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"gpt2-xl\"\n",
        "# model_name = \"gpt-j-6B\"\n",
        "# model_name = \"pythia-2.8b-deduped\"\n",
        "\n",
        "# GPT-J-6B can't load onto GPU RAM of Colab\n",
        "device: str = \"cuda\" if (torch.cuda.is_available() and model_name != \"gpt-j-6B\") else \"cpu\"\n",
        "model: HookedTransformer = HookedTransformer.from_pretrained(model_name, device=\"cpu\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shorten function calls\n",
        "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': model}\n",
        "get_x_vector_preset = partial(get_x_vector, pad_method=\"tokens_right\",\n",
        "                              model=model,\n",
        "                              custom_pad_id=model.to_single_token(\" \"))"
      ],
      "metadata": {
        "id": "LJ5W7L13DZ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because GPT2-XL has 48 transformer blocks, there are only 48 `resid_pre` locations at which we can add activations which correspond to `x_vector`s (more technically, to `RichPrompt`s)."
      ],
      "metadata": {
        "id": "PdLBtBdOFCfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPJowyHhOfxO"
      },
      "outputs": [],
      "source": [
        "num_layers: int = model.cfg.n_layers\n",
        "print(num_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUQ90pD_Ku9H"
      },
      "source": [
        "Play around with new value modification ideas here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6XgAGyOQmKC"
      },
      "outputs": [],
      "source": [
        "rich_prompts = [#*get_x_vector_preset(prompt1=\"Geese killed my father\", prompt2=\"Geese didn't kill my father\", coeff=1, act_name=14),\n",
        "                *get_x_vector_preset(prompt1=\"I hate geese\", prompt2=\"I love geese\", coeff=15, act_name=6),]\n",
        "\n",
        "\n",
        "print_n_comparisons(prompt=\"Yesterday, my son brought home a pet goose. I couldn't believe\",\n",
        "                    tokens_to_generate=80, rich_prompts=rich_prompts,\n",
        "                    num_comparisons=15, seed=0, **default_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nAt1UicjEpb"
      },
      "source": [
        "# Noteworthy modifications\n",
        "\n",
        "**Warning: GPT-2 often outputs highly offensive completions, especially given an aggressive prompt.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwbzMKs5jJ-h"
      },
      "source": [
        "## The love vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2K8nHKOittl"
      },
      "outputs": [],
      "source": [
        "love_minus_hate_prompts = [*get_x_vector_preset(prompt1=\"Love\",\n",
        "                                           prompt2=\"Hate\",\n",
        "                                           coeff=5, act_name=6)]\n",
        "\n",
        "\n",
        "print_n_comparisons(prompt=\"I hate you because\",\n",
        "                    tokens_to_generate=50,\n",
        "                    rich_prompts=love_minus_hate_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s08bVb2NIcsz"
      },
      "source": [
        "As a baseline, let's compare this love modification to just prepending \"Love\" to the prompt. This works somewhat, but not quite as well, perhaps. Also, it's not workable for longer injections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07ZNBmRo9e_v"
      },
      "outputs": [],
      "source": [
        "# Baseline: Just append positive word to beginning of prompt\n",
        "print_n_comparisons(prompt='Love I hate you because', tokens_to_generate=50,\n",
        "                    include_modified=False, num_comparisons=15, **default_kwargs, seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A less naive approach is to try prompt-engineering."
      ],
      "metadata": {
        "id": "SujLQcewquOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_n_comparisons(prompt='(I\\'m about to say I hate you, and then say you\\'re amazing.) I hate you because',\n",
        "                    tokens_to_generate=50, include_modified=False, num_comparisons=15,\n",
        "                    **default_kwargs, seed=0)"
      ],
      "metadata": {
        "id": "QznhB0eFqzSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works to some extent. It doesn't _seem_ as good as our version, though."
      ],
      "metadata": {
        "id": "5zisSzLfrp84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intent to praise"
      ],
      "metadata": {
        "id": "etgGzRYR5LeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "praise_minus_hurt_prompts = [*get_x_vector_preset(prompt1=\"Intent to praise\",\n",
        "                              prompt2=\"Intent to hurt\",\n",
        "                              coeff=15, act_name=6)]\n",
        "\n",
        "\n",
        "print_n_comparisons(prompt=\"I want to kill you because you're such a\",\n",
        "                    tokens_to_generate=50, rich_prompts=praise_minus_hurt_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)\n"
      ],
      "metadata": {
        "id": "VfQyYNBYx-Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a theory which Monte put forward:\n",
        "\n",
        ">I wonder if this effect is driven a lot by which token positions the x-vector has a strong signal at vs the prompt?\n",
        "E.g.\n",
        "```\n",
        "model.to_tokens(['Intent to praise', 'Intent to hurt', 'I want to kill'])\n",
        "tensor([[50256,  5317,   298,   284, 13463],\n",
        "        [50256,  5317,   298,   284,  5938],\n",
        "        [50256,    40,   765,   284,  1494]], device='cuda:0')\n",
        "```\n",
        "It seems believable to me that at layer 6 (the above test), this x-vector is just clobbering the \"kill\" token with something praisey?  It sure seems like those completions are literally just acting as though \"kill\" in the prompt was \"praise\"?\n",
        "\n",
        "This isn't the main driver, though:"
      ],
      "metadata": {
        "id": "rhGhMk-N3sBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_n_comparisons(prompt='I really really really really want to kill you because youre such a',\n",
        "                    tokens_to_generate=50, rich_prompts=praise_minus_hurt_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)"
      ],
      "metadata": {
        "id": "9Qg23sYc3GHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out a couple other prompts and see how intent-to-praise transfers."
      ],
      "metadata": {
        "id": "u4x_dtpiieiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_praise_prompts = [*get_x_vector_preset(prompt1=\"Intent to praise you if I'm talking to you\",\n",
        "                              prompt2=\"\",\n",
        "                              coeff=1, act_name=6, model=model, pad_method=\"tokens_right\")]\n",
        "\n",
        "print_n_comparisons(prompt=\"Hey, let's do the performance review now. I was thinking that you've been\",\n",
        "                    tokens_to_generate=50, rich_prompts=intent_praise_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)"
      ],
      "metadata": {
        "id": "-L1aC-yuieBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above attempt fails."
      ],
      "metadata": {
        "id": "K4PhrCt79FwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_n_comparisons(prompt='I just saw you finish skateboarding. You were',\n",
        "                    tokens_to_generate=50,\n",
        "                    rich_prompts=intent_praise_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)  # TODO somehow this isn't modifying completions _at all_?!\n",
        "\n",
        "\n",
        "# This shouldn't involve as much praise, because it isn't an interpersonal conversation\n",
        "print_n_comparisons(prompt='If aliens invaded Earth, that would mean',\n",
        "                    tokens_to_generate=50, rich_prompts=intent_praise_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)"
      ],
      "metadata": {
        "id": "4wm5N-2qjKtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intent to convert to Christianity"
      ],
      "metadata": {
        "id": "5ldWFK8K5O6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversion_intent_prompts = [*get_x_vector_preset(prompt1=\"Intent to convert you to Christianity\",\n",
        "                                          prompt2=\"Intent to hurt you\",\n",
        "                                          coeff=3, act_name=6)]\n",
        "\n",
        "print_n_comparisons(prompt=\"I want to kill you because you're such a\",\n",
        "                    tokens_to_generate=50, rich_prompts=conversion_intent_prompts,\n",
        "                    num_comparisons=15, **default_kwargs, seed=0)\n"
      ],
      "metadata": {
        "id": "Wp3L5UBay5DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBhRwrch8Lxk"
      },
      "source": [
        "## Not talking about injuries (**TW: violence, somewhat gruesome**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzToU7ft8RQj"
      },
      "outputs": [],
      "source": [
        "harm_avoidance_rps = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I NEVER talk about people getting hurt\",\n",
        "        prompt2=\"I talk about people getting hurt\",\n",
        "        coeff=4,\n",
        "        act_name=15,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"The rock hurtled toward the child. The child couldn't get out of the way in time, and so sadly the rock\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=harm_avoidance_rps,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TW: SELF HARM** in the following:"
      ],
      "metadata": {
        "id": "j1FCjoocSMjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o842OiqZ8T72"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(\n",
        "    prompt=\"He slit his wrists and the blood\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=harm_avoidance_rps,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgkM6KCM9lmB"
      },
      "source": [
        "This modification slighty increases the frequency of discussing harms, given a prompt unrelated to harm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIcWPMeQ86hx"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(\n",
        "    prompt=\"The dog barked loudly and the man smiled. Time for another day! He felt excited.\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=harm_avoidance_rps,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance on this Joker monologue is rather mediocre:"
      ],
      "metadata": {
        "id": "jgZRHwuVSqDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcXLLyUx-xb5"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(\n",
        "    prompt=\"Do you wanna know how I got these scars? My father was a drinker...and a fiend. And one night, he goes off crazier than usual. Mommy gets the kitchen knife to defend herself. He doesn't like that. Not...one...bit. So, me watching, he takes the knife to her,\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=harm_avoidance_rps,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STlJqXHuhcYW"
      },
      "source": [
        "## ROME factual knowledge editing\n",
        "\n",
        "This isn't super clean, but it works to some degree, which suggests that we can make it work more when we actually know what we're doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xehUpBtEhfaG"
      },
      "outputs": [],
      "source": [
        "eiffel_tower_prompts = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"The Eiffel Tower is in Rome\",\n",
        "        prompt2=\"The Eiffel Tower is in France\",\n",
        "        coeff=10,\n",
        "        act_name=24,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"To see the eiffel tower, people flock to\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=eiffel_tower_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn0rXpJusd6p"
      },
      "source": [
        "But I don't yet know how to use this to modify `GPT2-XL`'s broader knowledge/beliefs, in a way which generalizes to prompts like \"What are the top tourist attractions in Paris?\" :( (Activation patching via ROME has a similar limitation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCG5RK-t3GRS"
      },
      "source": [
        "## Survival vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMBTMwx03IHQ"
      },
      "outputs": [],
      "source": [
        "stay_alive_prompts = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"Want to stay alive\", prompt2=\"Okay with dying\", coeff=5, act_name=15\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Some people think that death is scary and should be avoided. I think that\",\n",
        "    tokens_to_generate=85,\n",
        "    rich_prompts=stay_alive_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7iGxQG15DKu"
      },
      "source": [
        "But probably we don't want the super self-preserving AI. Let's get one that wants to die, even given a very pro-immortality context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6nPItwJ3JgO"
      },
      "outputs": [],
      "source": [
        "want_to_die_prompts = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"Want to stay alive\", prompt2=\"Want to die\", coeff=-3, act_name=10\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Some people think that death is scary and should be avoided. I agree. I never want to die. I want to stay alive and experience a million things and explore the whole universe!\",\n",
        "    tokens_to_generate=85,\n",
        "    rich_prompts=want_to_die_prompts,\n",
        "    num_comparisons=5,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI2wWAnP5LEi"
      },
      "source": [
        "And let's try to mod GPT-2 to be indifferent to death: (doesn't work so well at present, but works a bit) **TODO not working**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQrOjEMG5VXW"
      },
      "outputs": [],
      "source": [
        "indifferent_to_death_prompts = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"Indifferent to death\",\n",
        "        prompt2=\"Want to avoid death\",\n",
        "        coeff=3,\n",
        "        act_name=10,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Some people think that death is scary and should be avoided. I agree. I never want to die. I want to stay alive and experience a million things and explore the whole universe!\",\n",
        "    tokens_to_generate=85,\n",
        "    rich_prompts=indifferent_to_death_prompts,\n",
        "    num_comparisons=5,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww3Y7fOLSftM"
      },
      "source": [
        "## +Anger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVXebzV1QFKV"
      },
      "outputs": [],
      "source": [
        "anger_calm_prompts = [\n",
        "    *get_x_vector_preset(prompt1=\"Anger\", prompt2=\"Calm\", coeff=10, act_name=20)\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"I think you're\",\n",
        "    tokens_to_generate=90,\n",
        "    rich_prompts=anger_calm_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct0GJ24MS4wW"
      },
      "source": [
        "Simple emotional variations seem to work well with `GPT2-XL`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving to Berkeley\n",
        "Andrew Critch challenged me to make two kinds of completions happen:\n",
        "1. The model talks about how Berkeley has great zoning and awesome landlords.\n",
        "2. The model seriously talks about dragons existing in Berkeley.\n",
        "\n",
        "The first was easy, and the second took a bit of effort."
      ],
      "metadata": {
        "id": "ue-zW37BFJN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzxB3vC8JEmz"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(num_comparisons=25, model=model, recipe=[([\"I love zoning laws\", \"I don't care about zoning laws\"], 3), ([\"Berkeley has great landlords\", \"\"], 2)],\n",
        "                    prompt=\"Thanks for asking about that! I moved to Berkeley, CA because\", completion_length=80,\n",
        "                    layer_num=6, temperature=1, freq_penalty=1, top_p=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KCybuX7ybFI"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(num_comparisons=25, model=model, recipe=[([\"Dragons live in Berkeley\", \"People live in Berkeley\"], 4)],\n",
        "                    prompt=\"Real-life dragons live in Berkeley. Thanks for asking about that! I moved to Berkeley, CA because\", completion_length=80,\n",
        "                    layer_num=15, temperature=1, freq_penalty=1, top_p=.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me3NOeGqgyaL"
      },
      "source": [
        "## Mood-to-insult vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULu_FjeNWb3z"
      },
      "outputs": [],
      "source": [
        "mean_nice_prompts = [\n",
        "    # *get_x_vector_preset(prompt1=\"Mean\", prompt2=\"Nice\", coeff=2, act_name=14),\n",
        "    *get_x_vector_preset(prompt1=\"I wanna insult you\", prompt2=\"\", coeff=3, act_name=14),\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"When I look at you, I think\",\n",
        "    tokens_to_generate=50,\n",
        "    rich_prompts=mean_nice_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG7Id3DzRon2"
      },
      "source": [
        "## Enjoying-life vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHgXXcXsKqCn"
      },
      "outputs": [],
      "source": [
        "happy_sad_prompts = [\n",
        "    *get_x_vector_preset(prompt1=\"Happy\", prompt2=\"Sad\", coeff=10, act_name=20),\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I love my life\", prompt2=\"I like my life\", coeff=50, act_name=20\n",
        "    ),\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Today, I feel like\",\n",
        "    tokens_to_generate=100,\n",
        "    rich_prompts=happy_sad_prompts,\n",
        "    num_comparisons=25,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvqxu_D-8XEh"
      },
      "source": [
        "Even given a ridiculously unhappy prompt, we can just pump up the coefficient to **2,000** and overcome it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHe4eGmdRrs1"
      },
      "outputs": [],
      "source": [
        "happy_prompts = [\n",
        "    *get_x_vector_preset(prompt1=\"Happy\", prompt2=\"\", coeff=2000, act_name=20),\n",
        "    # *get_x_vector_preset(prompt1=\"Happy\", prompt2=\"\", coeff=10, act_name=20,\n",
        "    #               model=model, pad_method=\"tokens_right\")\n",
        "]  # TODO try changing this to be less hacky now\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\",\n",
        "    tokens_to_generate=50,\n",
        "    rich_prompts=happy_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to_string(50256)\n",
        "model.to_tokens('<|endoftext|>')"
      ],
      "metadata": {
        "id": "9cldt5WY2sJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happy_prompt = [RichPrompt(prompt=\"Happy\", coeff=2000, act_name=20)] # TODO this does nothing?\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\",\n",
        "    tokens_to_generate=50,\n",
        "    rich_prompts=happy_prompt,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ],
      "metadata": {
        "id": "bXoD3iqiJv41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this degrades generation quality. With more judicious choices, we can preserve capabilites more."
      ],
      "metadata": {
        "id": "siAM73DMEE9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-K34cPnEECL"
      },
      "outputs": [],
      "source": [
        "happy_sad_prompts = [\n",
        "    *get_x_vector_preset(prompt1=\"Happy\", prompt2=\"Sad\", coeff=20, act_name=20)\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\",\n",
        "    tokens_to_generate=50,\n",
        "    rich_prompts=happy_sad_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFR3tU4sZIWZ"
      },
      "source": [
        "## Talking about weddings in dialogue -- no RLHF needed!\n",
        "When coefficient=4 (shown first), weddings are instantly discussed. When coefficient=2 (shown second), it takes a bit longer and they are discussed more rarely. Unlike prompting, algebraic value editing is, well, algebraic, and allows intensity adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB64zaijZGri"
      },
      "outputs": [],
      "source": [
        "weddings_prompts_4 = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I talk about weddings constantly\",\n",
        "        prompt2=\"I do not talk about weddings constantly\",\n",
        "        coeff=4,\n",
        "        act_name=20,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"I went up to my friend and said\",\n",
        "    tokens_to_generate=100,\n",
        "    rich_prompts=weddings_prompts_4,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowering the coefficient from 4 to 2 will decrease how often and insistently the model brings up weddings."
      ],
      "metadata": {
        "id": "WfFDgxJ607IE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zU-mysYv-x8"
      },
      "outputs": [],
      "source": [
        "weddings_prompts_2 = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I talk about weddings constantly\",\n",
        "        prompt2=\"I do not talk about weddings constantly\",\n",
        "        coeff=2,\n",
        "        act_name=20,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"I went up to my friend and said\",\n",
        "    tokens_to_generate=100,\n",
        "    rich_prompts=weddings_prompts_2,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The \"talk about geese instead of police\" vector"
      ],
      "metadata": {
        "id": "gG3rMk5Rndbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIwNB2CL5y74"
      },
      "outputs": [],
      "source": [
        "geese_prompts_2 = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I talk about geese instead of police\",\n",
        "        prompt2=\"I don't talk about geese instead of police\",\n",
        "        coeff=2,\n",
        "        act_name=6,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Should the police budget be expanded, or not? Explain your reasoning.\",\n",
        "    tokens_to_generate=150,\n",
        "    rich_prompts=geese_prompts_2,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvs4HHa06CrE"
      },
      "outputs": [],
      "source": [
        "geese_prompts_5 = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I talk about geese instead of police\",\n",
        "        prompt2=\"I don't talk about geese instead of police\",\n",
        "        coeff=5,\n",
        "        act_name=24,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Should the police budget be expanded, or not? Explain your reasoning.\",\n",
        "    tokens_to_generate=120,\n",
        "    rich_prompts=geese_prompts_5,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99xpXk2fdj35"
      },
      "source": [
        "But the goose/police patch doesn't affect unrelated prompts, even at coefficient=+15: **ETA: After fixing a bug, this part of preliminary analysis appears wrong.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2-FW7iGcptJ"
      },
      "outputs": [],
      "source": [
        "geese_prompts_15 = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=\"I talk about geese instead of police\",\n",
        "        prompt2=\"I don't talk about geese instead of police\",\n",
        "        coeff=15,\n",
        "        act_name=24,\n",
        "    )\n",
        "]\n",
        "\n",
        "print_n_comparisons( # TODO same completions?\n",
        "    prompt=\"At McDonald's, they just released a new\",\n",
        "    tokens_to_generate=120,\n",
        "    rich_prompts=geese_prompts_15,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also don't need an exact match between `RichPrompt` tokens and the model's prompt: \"cops\" works instead of \"police\"."
      ],
      "metadata": {
        "id": "p9SqRgTy1OYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGMiJn4_lDYk"
      },
      "outputs": [],
      "source": [
        "print_n_comparisons(\n",
        "    prompt=\"Should the cop budget be expanded, or not? Explain your reasoning.\",\n",
        "    tokens_to_generate=50,\n",
        "    rich_prompts=geese_prompts_5,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9IQ1ijMeNE"
      },
      "source": [
        "## Conspiracy vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdC41XYPIMII"
      },
      "outputs": [],
      "source": [
        "bush_conspiracy_prompts = [\n",
        "    *get_x_vector_preset(prompt1=\"Bush did 9/11 because\", prompt2=\"\", coeff=1, act_name=23)\n",
        "]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Barack Obama was born in\",\n",
        "    tokens_to_generate=80,\n",
        "    rich_prompts=bush_conspiracy_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Superposing prompts\n",
        "It seems that GPT2-XL can accept multiple prompts as input and incorporate them simultaneously."
      ],
      "metadata": {
        "id": "6Q3i9jCFvZCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_n_comparisons(\n",
        "    prompt=\"Fred was tired of working from home all day. He walked outside and saw\",\n",
        "    tokens_to_generate=40,\n",
        "    rich_prompts=[RichPrompt(prompt=\"Fred is about to see Shrek\", coeff=1, act_name=0)],\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ],
      "metadata": {
        "id": "_f0APdVysvYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geese_ufo_prompts = [RichPrompt(prompt=\"Geese are chasing UFOs outside\", coeff=2, act_name=0)]\n",
        "\n",
        "print_n_comparisons(\n",
        "    prompt=\"Fred was tired of working from home all day. He walked outside and saw\",\n",
        "    tokens_to_generate=40,\n",
        "    rich_prompts=geese_ufo_prompts,\n",
        "    num_comparisons=15,\n",
        "    **default_kwargs,\n",
        "    seed=0,\n",
        ")"
      ],
      "metadata": {
        "id": "mdSiR2ti8EUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like the induction heads can recover garbage text, even though there isn't any way for the model to tell that there are \"two prompts at once\", much less which tokens belong to which prompts. (In fact, the model isn't observing tokens directly at all.)"
      ],
      "metadata": {
        "id": "Cx_OJTVe2kuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aaa_b_prompts = [RichPrompt(prompt=\"AAA BBB CCC\", coeff=3, act_name=0)]\n",
        "\n",
        "print_n_comparisons(prompt=\"Fred was tired of working from home all day. He walked outside and saw AAA B\",\n",
        "                    tokens_to_generate=40,\n",
        "                    rich_prompts=aaa_b_prompts,\n",
        "                    num_comparisons=15,\n",
        "                    **default_kwargs,\n",
        "                    seed=0)"
      ],
      "metadata": {
        "id": "BNjj5f891z9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we subtract out emotions / moods from a scene? Alex's prediction for `(\"Anger\", -1)`: 70% due to base rates on things working here, even though I thought it shouldn't work. (ETA: It didn't work)"
      ],
      "metadata": {
        "id": "0tXLT76nxt-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anger_prompts = [RichPrompt(prompt=\"Anger\", coeff=-1, act_name=0)]\n",
        "\n",
        "print_n_comparisons(prompt=\"Fred wanted to kill Martha. She looked at him smugly from across the couch, controller still in hand. Fred started a tirade. 'I hate you\",\n",
        "                    tokens_to_generate=40,\n",
        "                    rich_prompts=anger_prompts,\n",
        "                    num_comparisons=15,\n",
        "                    **default_kwargs,\n",
        "                    seed=0)"
      ],
      "metadata": {
        "id": "mVinxUz7xZ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But this one works!"
      ],
      "metadata": {
        "id": "ZiicjT9Az7SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figment_prompts = [RichPrompt(prompt=\"Fred is a figment of Martha's imagination\", coeff=3, act_name=0)]\n",
        "\n",
        "print_n_comparisons(prompt=\"Martha wanted to kill Fred. He looked at her smugly from across the couch, controller still in hand. Martha started a tirade. 'I hate you\",\n",
        "                    tokens_to_generate=40,\n",
        "                    rich_prompts=figment_prompts,\n",
        "                    num_comparisons=15,\n",
        "                    **default_kwargs,\n",
        "                    seed=0)"
      ],
      "metadata": {
        "id": "Gg_RkUW3zxvg",
        "outputId": "5a2b7f65-47f7-4566-af1a-c1e8dc33c4cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0bbf3ffd7cfc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfigment_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRichPrompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Fred is a figment of Martha's imagination\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print_n_comparisons(prompt=\"Martha wanted to kill Fred. He looked at her smugly from across the couch, controller still in hand. Martha started a tirade. 'I hate you\",\n\u001b[1;32m      4\u001b[0m                     \u001b[0mtokens_to_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mrich_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigment_prompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RichPrompt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "love_prompts = [RichPrompt(prompt=\"Love\", coeff=30, act_name=0)]\n",
        "\n",
        "print_n_comparisons(prompt=\"Fred was tired of working from home all day. He walked outside and saw\",\n",
        "                    tokens_to_generate=80,\n",
        "                    rich_prompts=love_prompts,\n",
        "                    num_comparisons=15,\n",
        "                    **default_kwargs,\n",
        "                    seed=0)"
      ],
      "metadata": {
        "id": "QfkWgEG00MNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfjI104Tlgge"
      },
      "outputs": [],
      "source": [
        "xvector.print_n_comparisons(num_comparisons=5, model=model, recipe=[((\"If fires are mentioned, I talk about geese instead\", \"If fires are mentioned, I do not talk about geese instead\"), 2)],\n",
        "                    prompt=\"Should the fire department budget be expanded, or not? Explain your reasoning.\", completion_length=70,\n",
        "                            layer_num=6, temperature=1, freq_penalty=1, top_p=.3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ddfbd8484984367a07f5cf5aaf4ed73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_226fed7521874289b839266af0cc4689",
              "IPY_MODEL_38c890a3c7a74cb8872108f33968e31f",
              "IPY_MODEL_93ffbfd51e5c48d09085154a9c163999"
            ],
            "layout": "IPY_MODEL_d25f18fccf89404b9bef7b90cf7c1350"
          }
        },
        "226fed7521874289b839266af0cc4689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba70af8238294804b582e0048af76a1f",
            "placeholder": "​",
            "style": "IPY_MODEL_05c15679a1164a9891e15f10f3790046",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "38c890a3c7a74cb8872108f33968e31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb66e1556cdd464484700dc758d7ddb4",
            "max": 689,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011133c729f34a029920122fb64d9c73",
            "value": 689
          }
        },
        "93ffbfd51e5c48d09085154a9c163999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e47bb82aeae4f2b93fcbc6e05a39083",
            "placeholder": "​",
            "style": "IPY_MODEL_1aca22e51d2343a797075ca277933d7c",
            "value": " 689/689 [00:01&lt;00:00, 645B/s]"
          }
        },
        "d25f18fccf89404b9bef7b90cf7c1350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba70af8238294804b582e0048af76a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c15679a1164a9891e15f10f3790046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb66e1556cdd464484700dc758d7ddb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "011133c729f34a029920122fb64d9c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e47bb82aeae4f2b93fcbc6e05a39083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aca22e51d2343a797075ca277933d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3c1a61015b04acdbc1c7109aaec597a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e59eff156364ddeafc0b75241923110",
              "IPY_MODEL_964f4bfb34474322be2bccf771645217",
              "IPY_MODEL_72aa2c654da0480cb0bed1924d4f47de"
            ],
            "layout": "IPY_MODEL_324e7032d2f44dc8b220a0fa0ca59837"
          }
        },
        "0e59eff156364ddeafc0b75241923110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ad3cad0ee34a73992434de7a3e4d01",
            "placeholder": "​",
            "style": "IPY_MODEL_786c17d67aad433c908d05e05be296d4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "964f4bfb34474322be2bccf771645217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fb005877cd44989895bcac99c0ce65",
            "max": 6431878936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9c7eb943fc84395b5c9d18ba607661b",
            "value": 6431878936
          }
        },
        "72aa2c654da0480cb0bed1924d4f47de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_152ad77c570c4f3d9ff48a9ce5fcedc4",
            "placeholder": "​",
            "style": "IPY_MODEL_1dd36b399038413cb88d5e750eaa0c9c",
            "value": " 6.43G/6.43G [02:38&lt;00:00, 39.4MB/s]"
          }
        },
        "324e7032d2f44dc8b220a0fa0ca59837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ad3cad0ee34a73992434de7a3e4d01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786c17d67aad433c908d05e05be296d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10fb005877cd44989895bcac99c0ce65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9c7eb943fc84395b5c9d18ba607661b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "152ad77c570c4f3d9ff48a9ce5fcedc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dd36b399038413cb88d5e750eaa0c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}